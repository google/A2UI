{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Personalized Learning Demo - Quickstart\n\nA full-stack A2UI sample demonstrating personalized educational content generation.\n\n**Contributed by Google Public Sector's Rapid Innovation Team.**\n\n---\n\n## What You'll Learn\n\nThis demo showcases several key concepts for building agentic applications with A2UI:\n\n| Concept | What This Demo Shows |\n|---------|---------------------|\n| **Remote Agent Deployment** | Deploy an AI agent to Vertex AI Agent Engine that runs independently from your UI |\n| **A2A Protocol** | Use the Agent-to-Agent protocol to communicate between your frontend and the remote agent |\n| **Custom UI Components** | Extend A2UI with custom components (Flashcard, QuizCard) beyond the standard library |\n| **Dynamic Content Generation** | Generate personalized A2UI JSON on-the-fly based on user requests |\n| **Intelligent Content Matching** | Use LLMs to match user topics to relevant textbook content (167 OpenStax chapters) |\n\n### Why This Architecture Matters\n\nIn production agentic systems:\n- **Agents run remotely** - They scale independently, can be updated without redeploying the UI, and may be operated by third parties\n- **UI is decoupled** - The frontend renders whatever A2UI JSON the agent sends, without knowing the agent's implementation\n- **A2A enables interoperability** - Any A2A-compatible agent can power your UI, regardless of how it's built\n\nThis demo implements this full stack: a TypeScript/Lit frontend communicates via A2A with a Python agent running on Google Cloud.\n\n---\n\n## About This Notebook\n\nThis Jupyter notebook walks you through setting up and running the Personalized Learning demo. It's designed to \"just work\" - run each cell in order and you'll have a working demo.\n\n### What This Demo Shows\n\nThis demo showcases how A2UI enables AI agents to generate rich, interactive learning materials tailored to individual learners:\n\n- **Flashcards** - Generated from OpenStax textbook content\n- **Audio** - Personalized podcasts (via NotebookLM)\n- **Video** - Educational explainers\n- **Quizzes** - Interactive assessment\n\n### The Personalization Pipeline\n\nAt Google Public Sector, we're developing approaches that combine LLMs, knowledge graphs, and learner performance data to produce personalized content across courses—and across a person's academic and professional life.\n\nFor this demo, that personalization is represented by context files in `learner_context/` describing a fictional learner (Maria) and her learning needs.\n\n---\n\n## How This Notebook Is Organized\n\n| Section | What It Does |\n|---------|--------------|\n| **Step 1: Environment Setup** | Creates Python virtual environment and installs all dependencies |\n| **Step 2: Configuration** | Sets your GCP project ID |\n| **Step 3: GCP Authentication** | Authenticates with Google Cloud and enables required APIs |\n| **Step 4: Deploy Agent** | Deploys the AI agent to Vertex AI Agent Engine |\n| **Step 5: Configure & Run** | Creates config files and launches the demo |\n| **Step 6 (Optional)** | Generate audio/video content with NotebookLM |\n| **Appendix: Local Development** | Run entirely locally without cloud deployment |\n\n### Remote vs. Local Deployment\n\n**The default flow deploys the agent to Google Cloud.** This is intentional - the whole point of A2UI is to work with *remote* agents. In production, your UI runs in a browser while the agent runs on a server (possibly operated by a third party). This architecture enables:\n\n- Agents that scale independently of the UI\n- Agents that can be updated without redeploying the frontend\n- Multi-tenant scenarios where one agent serves many users\n- Integration with agents you don't control\n\nHowever, for quick local testing or if you don't have a GCP project, see **Appendix: Local Development** at the end of this notebook.\n\n---\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- **Node.js 18+** - [Download](https://nodejs.org/)\n- **Python 3.11+** - [Download](https://www.python.org/downloads/)\n- **Google Cloud project with billing enabled** - [Console](https://console.cloud.google.com/)\n- **gcloud CLI installed** - [Install Guide](https://cloud.google.com/sdk/docs/install)"
  },
  {
   "cell_type": "markdown",
   "source": "## Imports\n\nRun this cell first to load all Python modules used throughout the notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport sys\nimport os",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "First, we'll create an isolated Python environment and install all dependencies. This ensures the demo doesn't conflict with other Python projects on your system.\n",
    "\n",
    "### 1a. Create Python Virtual Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create virtual environment if it doesn't exist\nvenv_path = os.path.join(os.getcwd(), \".venv\")\nif not os.path.exists(venv_path):\n    print(\"Creating Python virtual environment...\")\n    subprocess.run([sys.executable, \"-m\", \"venv\", \".venv\"], check=True)\n    print(f\"✅ Created virtual environment at {venv_path}\")\nelse:\n    print(f\"✅ Virtual environment already exists at {venv_path}\")\n\nprint(\"\\n⚠️  IMPORTANT: Restart your Jupyter kernel to use the new environment!\")\nprint(\"   In VS Code: Click the kernel selector (top right) → Select '.venv'\")\nprint(\"   In JupyterLab: Kernel → Change Kernel → Python (.venv)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Install Python Dependencies\n",
    "\n",
    "After selecting the `.venv` kernel, run this cell to install all required Python packages.\n",
    "\n",
    "**Note:** We explicitly use `https://pypi.org/simple/` to ensure packages come from the official Python Package Index, avoiding issues with corporate proxies or custom registries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install Python dependencies using the canonical PyPI index\nprint(\"Installing Python dependencies from PyPI...\")\npackages = [\n    \"google-adk>=0.3.0\",\n    \"google-genai>=1.0.0\",\n    \"google-cloud-storage>=2.10.0\",\n    \"python-dotenv>=1.0.0\",\n    \"litellm>=1.0.0\",\n    \"vertexai\",\n]\n\nsubprocess.run([\n    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"--index-url\", \"https://pypi.org/simple/\",\n    \"--trusted-host\", \"pypi.org\",\n    \"--trusted-host\", \"files.pythonhosted.org\",\n    *packages\n], check=True)\n\nprint(\"✅ Python dependencies installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Install Node.js Dependencies\n",
    "\n",
    "Now we'll install the frontend dependencies. This includes the A2UI renderer library and the demo's own packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build the A2UI Lit renderer (using public npm registry)\nprint(\"Building A2UI Lit renderer...\")\nsubprocess.run(\n    \"npm install --registry https://registry.npmjs.org/ && npm run build\",\n    shell=True,\n    cwd=\"../../renderers/lit\",\n    check=True\n)\nprint(\"✅ A2UI renderer built\")\n\n# Install demo dependencies\nprint(\"\\nInstalling demo dependencies...\")\nsubprocess.run(\n    \"npm install --registry https://registry.npmjs.org/\",\n    shell=True,\n    check=True\n)\nprint(\"✅ Demo dependencies installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "Set your Google Cloud project ID below. This is the project where the agent will be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"your-project-id\"  # <-- CHANGE THIS to your GCP project ID\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: GCP Authentication & API Setup\n",
    "\n",
    "Authenticate with Google Cloud and enable the required APIs. This will open browser windows for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "!gcloud auth login\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable required APIs\n",
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable cloudbuild.googleapis.com\n",
    "!gcloud services enable storage.googleapis.com\n",
    "!gcloud services enable cloudresourcemanager.googleapis.com\n",
    "\n",
    "# Create staging bucket for Agent Engine (if it doesn't exist)\n",
    "!gsutil mb -l {LOCATION} gs://{PROJECT_ID}_cloudbuild 2>/dev/null || echo \"Bucket already exists\"\n",
    "\n",
    "print(\"\\n✅ GCP APIs enabled and staging bucket ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy the A2UI Agent\n",
    "\n",
    "The agent generates personalized learning content and runs on Vertex AI Agent Engine. Deployment takes 2-5 minutes.\n",
    "\n",
    "**Why deploy remotely?** A2UI is designed for remote agents - your UI runs in the browser while the agent runs on a server. This mirrors real-world architectures where agents scale independently and may even be operated by third parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Deploy the agent to Vertex AI Agent Engine (takes 2-5 minutes)\nprint(\"Deploying agent to Vertex AI Agent Engine...\")\nprint(\"This takes 2-5 minutes. Watch for the Resource ID at the end.\\n\")\n\nresult = subprocess.run(\n    [sys.executable, \"deploy.py\", \"--project\", PROJECT_ID, \"--location\", LOCATION],\n    cwd=\"agent\"\n)\n\nif result.returncode != 0:\n    print(\"\\n❌ Deployment failed. Check the error messages above.\")\nelse:\n    print(\"\\n✅ Deployment complete! Copy the Resource ID from the output above.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure & Run\n",
    "\n",
    "Fill in the Resource ID from the deployment output above, then create the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get your project NUMBER (different from project ID)\nresult = subprocess.run(\n    [\"gcloud\", \"projects\", \"describe\", PROJECT_ID, \"--format=value(projectNumber)\"], \n    capture_output=True, text=True\n)\nPROJECT_NUMBER = result.stdout.strip()\nprint(f\"Project Number: {PROJECT_NUMBER}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the Resource ID from the deployment output in Step 4\n",
    "AGENT_RESOURCE_ID = \"YOUR_RESOURCE_ID_HERE\"  # <-- PASTE YOUR RESOURCE ID HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "env_content = f\"\"\"# Generated by Quickstart.ipynb\n",
    "GOOGLE_CLOUD_PROJECT={PROJECT_ID}\n",
    "AGENT_ENGINE_PROJECT_NUMBER={PROJECT_NUMBER}\n",
    "AGENT_ENGINE_RESOURCE_ID={AGENT_RESOURCE_ID}\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"Created .env file:\")\n",
    "print(env_content)\n",
    "print(\"✅ Configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Demo\n",
    "\n",
    "Everything is set up! Run this command in your terminal (not in the notebook):\n",
    "\n",
    "```bash\n",
    "npm run dev\n",
    "```\n",
    "\n",
    "Then open **http://localhost:5174**\n",
    "\n",
    "### Try These Prompts\n",
    "\n",
    "| Prompt | What Happens |\n",
    "|--------|-------------|\n",
    "| \"Help me understand ATP\" | Generates flashcards from OpenStax |\n",
    "| \"Quiz me on bond energy\" | Interactive quiz cards |\n",
    "| \"Play the podcast\" | Audio player (requires Step 6) |\n",
    "| \"Show me a video\" | Video player (requires Step 6) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6 (Optional): Generate Audio & Video with NotebookLM\n",
    "\n",
    "The demo includes audio and video players, but you need to generate the media files. NotebookLM can create personalized podcasts based on the learner context.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A Google account with access to [NotebookLM](https://notebooklm.google.com/)\n",
    "- The `learner_context/` files from this demo\n",
    "\n",
    "---\n",
    "\n",
    "### Part A: Generate a Personalized Podcast\n",
    "\n",
    "**1. Create a NotebookLM Notebook**\n",
    "\n",
    "Go to [notebooklm.google.com](https://notebooklm.google.com/) and create a new notebook.\n",
    "\n",
    "**2. Upload Learner Context Files**\n",
    "\n",
    "Upload all files from the `learner_context/` directory:\n",
    "- `01_maria_learner_profile.txt` - Maria's background and learning preferences  \n",
    "- `02_chemistry_bond_energy.txt` - Bond energy concepts\n",
    "- `03_chemistry_thermodynamics.txt` - Thermodynamics content\n",
    "- `04_biology_atp_cellular_respiration.txt` - ATP and cellular respiration\n",
    "- `05_misconception_resolution.txt` - Common misconceptions to address\n",
    "- `06_mcat_practice_concepts.txt` - MCAT-focused content\n",
    "\n",
    "These files give NotebookLM the context to generate personalized content.\n",
    "\n",
    "**3. Generate the Audio Overview**\n",
    "\n",
    "- Click **Notebook guide** in the right sidebar\n",
    "- Click **Audio Overview** → **Generate**\n",
    "- Wait for generation to complete (typically 2-5 minutes)\n",
    "- NotebookLM will create a podcast-style discussion about the uploaded content\n",
    "\n",
    "**4. Customize the Audio (Optional)**\n",
    "\n",
    "Before generating, you can click **Customize** to provide specific instructions:\n",
    "\n",
    "```\n",
    "Create a podcast for Maria, a pre-med student preparing for the MCAT. \n",
    "Use gym and fitness analogies since she loves working out.\n",
    "Focus on explaining why \"energy stored in bonds\" is a misconception.\n",
    "Make it conversational and engaging, about 5-7 minutes long.\n",
    "```\n",
    "\n",
    "**5. Download and Install the Podcast**\n",
    "\n",
    "- Once generated, click the **⋮** menu on the audio player\n",
    "- Select **Download**\n",
    "- Save the file as `podcast.m4a`\n",
    "- Copy to the demo's assets directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your downloaded podcast to the assets directory\n",
    "# Replace ~/Downloads/podcast.m4a with your actual download path\n",
    "!cp ~/Downloads/podcast.m4a public/assets/podcast.m4a\n",
    "\n",
    "# Verify the file was copied\n",
    "!ls -la public/assets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part B: Create a Video (Two Options)\n",
    "\n",
    "#### Option 1: NotebookLM Briefing Document + Screen Recording\n",
    "\n",
    "**1. Generate a Briefing Document**\n",
    "\n",
    "In NotebookLM with your learner context loaded:\n",
    "- Click **Notebook guide** → **Briefing doc**\n",
    "- This creates a structured summary you can use as a video script\n",
    "\n",
    "**2. Create the Video**\n",
    "\n",
    "Use the briefing document to create a video:\n",
    "- **Screen recording** - Record yourself walking through the concepts using slides or a whiteboard app\n",
    "- **AI video tools** - Use tools like Synthesia, HeyGen, or similar to generate a video from the script\n",
    "- **Slide presentation** - Create slides and record with voiceover using QuickTime, Loom, or similar\n",
    "\n",
    "**3. Export and Install**\n",
    "\n",
    "Export your video as MP4 and copy to the assets directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your video to the assets directory\n",
    "# Replace ~/Downloads/demo.mp4 with your actual file path\n",
    "!cp ~/Downloads/demo.mp4 public/assets/demo.mp4\n",
    "\n",
    "# Verify both media files are in place\n",
    "!ls -la public/assets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Use Placeholder/Stock Content\n",
    "\n",
    "For demo purposes, you can use any MP4 video file. Rename it to `demo.mp4` and place it in `public/assets/`.\n",
    "\n",
    "---\n",
    "\n",
    "### Verify Media Files\n",
    "\n",
    "After copying your files, verify they're accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Media files status:\")\nprint(\"-\" * 40)\n\npodcast_path = \"public/assets/podcast.m4a\"\nvideo_path = \"public/assets/demo.mp4\"\n\nif os.path.exists(podcast_path):\n    size_mb = os.path.getsize(podcast_path) / (1024 * 1024)\n    print(f\"✅ Podcast: {podcast_path} ({size_mb:.1f} MB)\")\nelse:\n    print(f\"❌ Podcast: {podcast_path} NOT FOUND\")\n    \nif os.path.exists(video_path):\n    size_mb = os.path.getsize(video_path) / (1024 * 1024)\n    print(f\"✅ Video: {video_path} ({size_mb:.1f} MB)\")\nelse:\n    print(f\"❌ Video: {video_path} NOT FOUND\")\n\nprint(\"-\" * 40)\nprint(\"\\nRun 'npm run dev' and try:\")\nprint('  • \"Play the podcast\" - to hear the audio')\nprint('  • \"Show me a video\" - to watch the video')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Content Attribution\n",
    "\n",
    "### OpenStax\n",
    "\n",
    "Educational content is sourced from [OpenStax](https://openstax.org/), licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Specifically: [Biology for AP® Courses](https://openstax.org/details/books/biology-ap-courses) - OpenStax, Rice University\n",
    "\n",
    "---\n",
    "\n",
    "## Security Notice\n",
    "\n",
    "> **Warning:** When building production applications, treat any agent outside your control as potentially untrusted. This demo connects to Agent Engine within your own GCP project. Always review agent code before deploying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Local Development (No Cloud Required)\n",
    "\n",
    "If you don't have a GCP project or want to iterate quickly without deploying, you can run everything locally. The agent runs as a local Python server instead of on Vertex AI.\n",
    "\n",
    "**Important:** While local development is convenient for testing, remember that A2UI is designed for *remote* agents. The default cloud deployment flow demonstrates the real-world architecture where:\n",
    "- The UI runs in the browser\n",
    "- The agent runs on a remote server\n",
    "- They communicate via the A2A protocol\n",
    "\n",
    "Local development is a shortcut for testing - production deployments should use remote agents.\n",
    "\n",
    "### Local Setup\n",
    "\n",
    "If you haven't already completed Step 1, run the environment setup cells first. Then configure your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file for local development\n",
    "# You'll need a Google AI API key from https://aistudio.google.com/apikey\n",
    "\n",
    "GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\"  # <-- CHANGE THIS\n",
    "\n",
    "env_content = f\"\"\"# Generated by Quickstart.ipynb (Local Development Mode)\n",
    "GOOGLE_API_KEY={GOOGLE_API_KEY}\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "# Also create agent/.env for the local agent server\n",
    "with open(\"agent/.env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"Created .env files for local development\")\n",
    "print(\"\\n⚠️  Make sure to replace YOUR_API_KEY_HERE with your actual API key!\")\n",
    "print(\"   Get one at: https://aistudio.google.com/apikey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locally\n",
    "\n",
    "With the local `.env` configured, run all three servers (frontend, API proxy, and local agent):\n",
    "\n",
    "```bash\n",
    "npm run start:all\n",
    "```\n",
    "\n",
    "This starts:\n",
    "- **Vite dev server** at http://localhost:5174 (frontend)\n",
    "- **API server** at http://localhost:8080 (proxy)\n",
    "- **Agent server** at http://localhost:8081 (local Python agent)\n",
    "\n",
    "Open **http://localhost:5174** to use the demo.\n",
    "\n",
    "### Differences from Cloud Deployment\n",
    "\n",
    "| Aspect | Cloud (Default) | Local |\n",
    "|--------|-----------------|-------|\n",
    "| Agent location | Vertex AI Agent Engine | Local Python process |\n",
    "| Scalability | Auto-scales | Single process |\n",
    "| API key | Uses ADC/service account | Requires GOOGLE_API_KEY |\n",
    "| Startup time | ~5 min (one-time deploy) | Instant |\n",
    "| Cost | Vertex AI pricing | Free (API usage only) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}